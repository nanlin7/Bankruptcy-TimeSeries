---
title: "Time Series Final Project - Canada Bankruptcy Rate"
output: pdf_document
author: Nan Lin
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,cache=T)
```

This report is about using time series forecasting in R to analyze Canada Bankruptcy rates between 1987 and 2014

```{r include=F, message=F,warning=F}

library(readr)
library(forecast)
library(tseries)
library(lawstat)
library(hydroGOF)
library(car)
```

First of all, I plotted out the time series data after converting the data to time series object.

```{r warning=F, message=F,echo=F}
# read in data
canada <- read_csv('Data/train.csv')
canada_bk <- ts(canada$Bankruptcy_Rate, start=c(1987,1),end=c(2014,12),frequency=12)
par(mfrow=c(2,1))
plot(canada_bk,main = "Canada Bankruptcy Rate Per Month", xlab = "Year", ylab = 'Bankruptcy Rates')
plot(log(canada_bk),main = "Canada Log Bankruptcy Rate Per Month", xlab = "Year", ylab = 'Bankrupty Rates')

```

As we can see there was a strong growth from 1987 to roughly 2008. There is a spike in 2018 and a the sharp drop afterwards, which can be explained by Finance crisis around that time. We can also see there is a strong seasonality which is obvious for monthly bankruptcy rates. By applying log transformation, it will stablize the variance, hence transform expenential trend into a linear one. 


```{r echo=F}
# split data: train:validation = 8:2
# 2018 is the threshold
train <- canada_bk[time(canada_bk) < 2009]
vad <- canada_bk[time(canada_bk) >= 2009]

bk.train <- ts(train,start=c(1987,1),end=c(2008,12),frequency=12)
bk.vad <- ts(vad,start=c(2009,1),end=c(2014,12),frequency=12)

# take logs
bk.train.log <- log(bk.train)
bk.vad.log <- log(bk.test)

plot(stl(bk.train.log,s.window="periodic"))

```

Taking 2008 as the threshold for training/test split, next step is to focus on the training data to select parameters for the optimal model with highest predictive accuracy.

For training data, after decompositing into trend, seasonality and random noises individually, we have a good sense of how this time series behaves.

From this chart, we can see that the seasonality is strong and consistent. Obviously, the original time series is not stationary. 

Differencing computes the differences between consecutive observations. To remove the trend and seasonality, we can difference the time series data.

Differencing with lag 1: 

```{r echo=F}

acf(bk.train.log,lag.max = 60)
pacf(bk.train.log,lag.max = 60)

# base line - auto.arima
# ARIMA(1,1,2)(0,0,2)[12] 
auto.arima(bk.train.log)

# Check ordinary diff =1 
ndiffs(bk.train.log)
bk.train.log.d1 <- diff(bk.train.log,1)
acf(bk.train.log.d1,lag.max = 72)
pacf(bk.train.log.d1,lag.max = 72)
# check staionary, p=0.01, it's stationary d=1
adf.test(bk.train.log.d1)

# Check Seasonal diffs - not necessary D=0
ndiffs(bk.train.log.d1)

# from the acf and pacf above, we can see
# q<=5, Q<=6
# p<=4, P<=5
```


```{r echo=F, warning =F}
# Use RMSE as metric to loop through p, P, q, Q to choose the optimal parameter
if(TRUE){  
  RMSE_value <- c()
  p_par <- c()
  q_par <- c()
  P_par <- c()
  Q_par <- c()
  for (p in seq(0, 4, 1) ){
    for (q in seq(0, 5 ,1) ){
      for (P in seq(0, 5, 1) ){
        for (Q in seq(0, 6, 1) ){
            m <- arima(bk.train.log, order = c(p,1,q), seasonal = list(order = c(P,0,Q), period = 12), method = "CSS")
            m_pred <- forecast(m, h = 72, level=c(95))
            RMSE <- sqrt(mean((exp(m_pred[[4]]) - bk.vad)^2))
            RMSE_value <- c(RMSE_value, RMSE)
            p_par <- c(p_par, p)
            q_par <- c(q_par, q)
            P_par <- c(P_par, P)
            Q_par <- c(Q_par, Q)
          
        }
      }
    }
  }
  data.frame(p_par, q_par, P_par, Q_par, RMSE_value)
  index <- which(RMSE_value == min(RMSE_value))
  cat (p_par[index], q_par[index], P_par[index], Q_par[index])
}
```

By analyzing the ACF and PACF for differencing, we can try to fit the model by choosing p,d,q,P,D,Q,S paramsters in ARIMA((p,d,q),(P,D,Q),S) function.

- Seasonality: both ACF tail off at first seasonal lag, PACF tail off at first and second seasonal lag, suggesting seasonal P<=5 and Q<=6

- Non-seasonal: indicating p<=4, q<=5

After interation among these parameters, we selected the model with lowest RMSE. **p=2,q=4,P=5,Q=5**

```{r echo=F, warning=F}
m <- arima(bk.train.log, order = c(2,1,4), seasonal = list(order = c(5,0,5), period = 12), method = "CSS")
f <- forecast(m, h = 72, level=c(95))
cat ("RMSE =", sqrt(mean((exp(f[[4]]) - bk.vad)^2)), "\n" )

  # pred, h, l, fitted
l<-exp(ts(f$lower, start = c(2009, 1), frequency = 12))  #95% PI LL
h<-exp(ts(f$upper, start = c(2009, 1), frequency = 12)) #95% PI UL
pred<-exp(f$mean) #predictions
fitted <- exp(f$fitted)
  
  # Graphical format
par(mfrow=c(1,1))
plot(canada_bk, xlim=c(1987,2014), main = "Monthly Bankruptcy Rate", ylab = "Bankruptcy Rate", xlab = "Month")
abline(v = 2008, lwd = 2, col = "black")
points(pred, type = "l", col = "blue")
points(l, type = "l", col = "red")
points(h, type = "l", col = "red")
points(fitted, type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"), lty = 1, col = c("black", "green", "blue", "red"), cex = 0.5)
  
# residuals
e <- m$residuals
e <- ts(e, start=c(1987,1), frequency = 12)
par(mfrow=c(1,1))
plot(e, main="Residuals vs t", ylab="")
  
# test whether residuals have zero mean # fail: mean not equal to zero
t.test(e)
  
# test for heteroscedasticity
group <- c(rep(1,57), rep(2,57), rep(3,57), rep(4,57))
levene.test(e,group) #Levene
bartlett.test(e,group) #Bartlett   
  
# test for uncorrelatedness / randomness
tsdiag(m) #ACF and Ljung-Box test all in one!
# from ACF, no siginificance spikes in residuals!

# test for normality
par(mfrow=c(1,1))
qqnorm(e, main="QQ-plot of Residuals")
qqline(e, col = "red")
# reject the null hypothesis: normally distributed...
shapiro.test(e) #SW test

```

By looking at the residuals diagnostics,it looks like we have a workable model here since the residuals are not correlated as ACF are within 95 confidence interval, and no obvious pattern in residual plot. From the QQ-plot, we can see overall alignment with the theoretical normal distribution even though the existance of a heavy tail problem. After checking the assumption, now we can use this SARIMA(2,1,4)(5,1,5),s=12 to forecast and compare with held out test data.




